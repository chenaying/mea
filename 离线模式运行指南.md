# 离线模式运行指南（网络问题解决方案）

## 一、问题描述

当无法访问 Hugging Face 时，会出现以下错误：

```
OSError: We couldn't connect to 'https://huggingface.co' to load the files, and couldn't find them in the cached files.
```

## 二、解决方案

### 方案 1：使用离线模式 + 本地缓存（推荐）

如果之前已经下载过模型，Hugging Face 会将其缓存在本地。使用离线模式强制从缓存加载：

```bash
# 设置环境变量（临时）
export TRANSFORMERS_OFFLINE=1
export HF_DATASETS_OFFLINE=1

# 运行脚本
python viecap_inference_adapted.py \
    --offline_mode \
    --memory_id coco \
    --memory_caption_num 5 \
    --using_hard_prompt \
    --image_path ./images/instance1.jpg \
    --weight_path ./checkpoints/train_coco/coco_prefix-0014.pt
```

或者在运行时一起设置：

```bash
TRANSFORMERS_OFFLINE=1 HF_DATASETS_OFFLINE=1 python viecap_inference_adapted.py \
    --offline_mode \
    --memory_id coco \
    --memory_caption_num 5 \
    --using_hard_prompt \
    --image_path ./images/instance1.jpg \
    --weight_path ./checkpoints/train_coco/coco_prefix-0014.pt
```

### 方案 2：使用本地模型路径

如果模型已经复制到 `checkpoints/` 目录，代码会自动检测并使用本地路径。

#### 2.1 检查本地模型

确保以下模型目录存在：

```bash
# 检查模型目录
ls checkpoints/flan-t5-base-VG-factual-sg/
ls checkpoints/all-MiniLM-L6-v2/
```

如果存在，代码会自动使用这些路径。

#### 2.2 手动指定本地路径

如果需要使用其他位置的模型，可以直接指定路径：

```bash
python viecap_inference_adapted.py \
    --parser_checkpoint ./checkpoints/flan-t5-base-VG-factual-sg \
    --wte_model_path ./checkpoints/all-MiniLM-L6-v2 \
    --memory_id coco \
    --memory_caption_num 5 \
    --using_hard_prompt \
    --image_path ./images/instance1.jpg \
    --weight_path ./checkpoints/train_coco/coco_prefix-0014.pt
```

### 方案 3：下载模型到本地（如果有网络时）

如果有其他机器或网络环境可以访问 Hugging Face，可以预先下载模型：

```python
# 下载脚本 download_models.py
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from sentence_transformers import SentenceTransformer
import os

# 创建目录
os.makedirs('checkpoints', exist_ok=True)

# 下载 GPT2
print("Downloading GPT2...")
tokenizer = AutoTokenizer.from_pretrained('openai-community/gpt2')
tokenizer.save_pretrained('checkpoints/gpt2')

# 下载 Flan-T5
print("Downloading Flan-T5...")
parser_tokenizer = AutoTokenizer.from_pretrained('lizhuang144/flan-t5-base-VG-factual-sg')
parser_model = AutoModelForSeq2SeqLM.from_pretrained('lizhuang144/flan-t5-base-VG-factual-sg')
parser_tokenizer.save_pretrained('checkpoints/flan-t5-base-VG-factual-sg')
parser_model.save_pretrained('checkpoints/flan-t5-base-VG-factual-sg')

# 下载 SentenceBERT
print("Downloading SentenceBERT...")
wte_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
wte_model.save('checkpoints/all-MiniLM-L6-v2')

# 下载 CLIP
print("Downloading CLIP...")
# CLIP 会由 models.clip_utils 自动下载
```

然后使用本地路径运行。

### 方案 4：使用 Hugging Face 缓存目录

Hugging Face 的默认缓存目录通常是 `~/.cache/huggingface/`。如果缓存中存在模型，可以：

1. **查找缓存位置**：

```python
from transformers import file_utils
print(file_utils.default_cache_path)
# 通常是 ~/.cache/huggingface/transformers/
```

2. **设置缓存路径**（如果模型在其他位置）：

```bash
export HF_HOME=/path/to/huggingface/cache
export TRANSFORMERS_CACHE=/path/to/huggingface/transformers
```

3. **使用离线模式**：

```bash
TRANSFORMERS_OFFLINE=1 python viecap_inference_adapted.py --offline_mode ...
```

## 三、需要的模型文件

运行 MeaCap InvLM 需要以下模型：

| 模型 | Hugging Face ID | 本地路径示例 |
|------|----------------|-------------|
| GPT-2 Tokenizer | `openai-community/gpt2` | `checkpoints/gpt2/` |
| Flan-T5 Parser | `lizhuang144/flan-t5-base-VG-factual-sg` | `checkpoints/flan-t5-base-VG-factual-sg/` |
| SentenceBERT | `sentence-transformers/all-MiniLM-L6-v2` | `checkpoints/all-MiniLM-L6-v2/` |
| CLIP (VL) | `openai/clip-vit-base-patch32` | 由 `models.clip_utils` 自动处理 |

## 四、代码更新说明

我已经更新了 `viecap_inference_adapted.py`，现在支持：

1. **自动检测本地路径**：如果 `checkpoints/` 目录中存在模型，会自动使用
2. **离线模式参数**：添加了 `--offline_mode` 参数
3. **本地路径支持**：可以指定本地路径作为模型参数

## 五、快速检查清单

在运行前，检查以下内容：

- [ ] 检查网络连接（如果需要下载）
- [ ] 检查模型缓存是否存在（`~/.cache/huggingface/transformers/`）
- [ ] 检查 `checkpoints/` 目录中的模型
- [ ] 设置环境变量（如果使用离线模式）
- [ ] 使用 `--offline_mode` 参数

## 六、常见错误解决

### 错误 1：找不到本地模型

```
OSError: Can't load tokenizer for './checkpoints/gpt2'
```

**解决**：
- 检查路径是否正确
- 确认模型文件已完整下载（需要 `tokenizer_config.json`、`vocab.json` 等文件）

### 错误 2：离线模式仍然尝试连接

```
ConnectionError: ...
```

**解决**：
- 确保设置了 `TRANSFORMERS_OFFLINE=1` 环境变量
- 确保使用了 `--offline_mode` 参数
- 检查模型是否在缓存中

### 错误 3：模型路径不正确

**解决**：
- 使用绝对路径
- 确认目录结构正确（需要包含配置文件）

## 七、推荐工作流程

1. **第一次运行（有网络）**：
   ```bash
   python viecap_inference_adapted.py ...
   ```
   让 Hugging Face 自动下载并缓存模型

2. **后续运行（无网络）**：
   ```bash
   TRANSFORMERS_OFFLINE=1 python viecap_inference_adapted.py --offline_mode ...
   ```

3. **完全离线**：
   - 预先下载所有模型到 `checkpoints/`
   - 使用本地路径参数运行

## 八、总结

**最简单的解决方案**：

```bash
# 如果之前下载过模型（在缓存中）
TRANSFORMERS_OFFLINE=1 python viecap_inference_adapted.py --offline_mode \
    --memory_id coco \
    --memory_caption_num 5 \
    --using_hard_prompt \
    --image_path ./images/instance1.jpg \
    --weight_path ./checkpoints/train_coco/coco_prefix-0014.pt
```

如果缓存中也没有模型，需要：
1. 找到有网络的机器
2. 预先下载所有需要的模型
3. 复制到目标机器
4. 使用本地路径运行

